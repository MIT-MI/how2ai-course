- date: 2/4
  title: >
    Week 1 <strong>Introduction</strong> <a href="lec1 - introduction.pdf">[slides]</a>
  slides:
  topics:
    - Course syllabus and requirements <br/>
    - Introduction to AI and AI research
  readings:
    - <a href="https://arxiv.org/abs/2209.03430">Foundations and Trends in Multimodal Machine Learning&#58; Principles, Challenges, and Open Questions</a> <br/>
    - <a href="https://arxiv.org/abs/1705.09406">Multimodal Machine Learning&#58; A Survey and Taxonomy</a> <br/>
    - <a href="https://arxiv.org/abs/1206.5538">Representation Learning&#58; A Review and New Perspectives</a> <br/>

- date: 2/6
  title: >
    Week 1 <strong>Introduction to AI Research</strong> <a href="lec1.2 - AI research.pdf">[slides]</a>
  slides:
  topics:
    - Introduction to AI and AI research <br/>
    - Generating ideas, reading and writing papers, AI experimentation
  readings:

- date: 2/11
  title: >
    Week 2 <strong>Foundation 1: Data, structure, information</strong> <a href="lec2 - data.pdf">[slides]</a>
  slides:
  topics:
    - Common data modalities <br/>
    - Data collection strategies <br/>
    - Training objectives and generalization
  readings:
    - <a href="https://www.science.org/doi/abs/10.1126/science.aaa8415">Machine learning&#58; Trends, Perspectives, and Prospects</a> <br/>
    - <a href="https://arxiv.org/abs/1206.5538">Representation Learning&#58; A Review and New Perspectives</a> <br/>
    - <a href="https://arxiv.org/abs/2209.03430">Foundations and Trends in Multimodal Machine Learning&#58; Principles, Challenges, and Open Questions</a> <br/>
    - <a href="https://arxiv.org/abs/2104.13478">Geometric Deep Learning&#58; Grids, Groups, Graphs, Geodesics, and Gauges</a> <br/>

- date: 2/14
  title: >
    Week 2 <strong>Foundation 2: Practical AI tools</strong> <a href="Debugging Tips.pdf">[slides]</a>
  slides:
  topics:
    - Getting started with PyTorch <br/>
    - Huggingface packages <br/>
    - Debugging machine learning models
  readings:
  - <a href="https://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks</a> <br/>
  - <a href="https://colab.research.google.com/drive/1EDsjYRrAiujUew0GRJ_hyVoNMsSDnlnx?usp=sharing">Fine-tuning a Code LLM on Custom Code on a single GPU</a> <br/>
  - <a href="https://colab.research.google.com/drive/1SoTu6gvYcLNDqPwNPTWmsSYF-l-UfHPx?usp=sharing">MAS.S60 Pytorch Introduction</a> <br/>

- date: 2/18
  title: >
    Week 3 <strong>No class, shifted President's day</strong>
  slides:
  topics:
  readings:

- date: 2/20
  title: >
    Week 3 <strong>Project proposal presentations</strong>
  slides:
  topics:
  readings:

- date: 2/25
  title: >
    Week 4 <strong>Foundation 3: Model architectures</strong> <a href="lec3 - models.pdf">[slides]</a>
  slides:
  topics:
    - Structure and invariances <br/>
    - Temporal sequence models <br/>
    - Spatial convolution models <br/>
    - Models for sets and graphs
  readings:
    - <a href="https://arxiv.org/abs/2104.13478">Geometric Deep Learning&#58; Grids, Groups, Graphs, Geodesics, and Gauges</a> <br/>
    - <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words&#58; Transformers for Image Recognition at Scale</a> <br/>
    - <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> <br/>
    - <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> <br/>
    - <a href="https://arxiv.org/abs/1703.06114">Deep Sets</a> <br/>
    - <a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a> <br/>

- date: 2/25
  title: >
    Week 4 <strong>Discussion 1: Learning and generalization</strong>
  slides:
  topics:
  readings:
    - <a href="https://arxiv.org/pdf/2410.09649">Learning the Bitter Lesson</a> <br/>
    - <a href="https://arxiv.org/pdf/2303.06173">Unifying Grokking and Double Descent</a> <br/>
    - <a href="https://arxiv.org/pdf/2209.01610">Generalization in Neural Networks</a> <br/>
    - <a href="https://arxiv.org/pdf/2306.11644">Textbooks are all you Need</a> <br/>
    - <a href="https://arxiv.org/pdf/2207.07528">A Conceptual Pipeline for Machine Learning</a> <br/>

- date: 3/4
  title: >
    Week 5 <strong>Multimodal 1: Connections and alignment</strong> <a href="lec4 - multimodal.pdf">[slides]</a>
  slides:
  topics:
    - Heterogeneity, connections, and interactions <br/>
    - Multimodal technical challenges <br/>
    - Alignment and transformers
  readings:
    - <a href="https://arxiv.org/abs/2209.03430">Foundations and Trends in Multimodal Machine Learning&#58; Principles, Challenges, and Open Questions</a> <br/>
    - <a href="https://arxiv.org/abs/2005.10243">What Makes for Good Views for Contrastive Learning?</a> <br/>
    - <a href="https://link.springer.com/article/10.1007/s13735-019-00187-6">Characterization and classification of semantic image-text relations</a> <br/>
    - <a href="https://arxiv.org/abs/2210.01936">When and why vision-language models behave like bags-of-words, and what to do about it?</a> <br/>

- date: 3/6
  title: >
    Week 5 <strong>Discussion 2: Modern AI architectures</strong>
  slides:
  topics:
  readings:
  - <a href="https://arxiv.org/abs/2301.03728">Scaling Laws for Generative Mixed-Modal Models</a> <br/>
  - <a href="https://arxiv.org/abs/2404.07965">Not All Tokens Are What You Need for Pretraining</a> <br/>
  - '<a href="https://arxiv.org/abs/2209.06794">PaLI: A Jointly-Scaled Multilingual Language-Image Model</a> <br/>'
  - <a href="https://arxiv.org/abs/2405.17927">The Evolution of Multimodal Model Architectures</a> <br/>
  - '<a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> <br/>'
  - <a href="https://arxiv.org/abs/2201.03545">A ConvNet for the 2020s</a> <br/>
  - <a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large Graphs</a> <br/>
  - '<a href="https://arxiv.org/abs/1811.01900">Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs</a> <br/>'

- date: 3/11
  title: >
    Week 6 <strong>Multimodal 2: Interactions and fusion</strong> <a href="lec5 - fusion.pdf">[slides]</a>
  slides:
  topics:
    - Cross-modal interactions <br/>
    - Multimodal fusion
  readings:
    - <a href="https://dl.acm.org/doi/pdf/10.1145/319382.319398">Ten Myths of Multimodal Interaction</a> <br/>
    - <a href="https://www.sciencedirect.com/science/article/pii/S0167865513002584">Multimodal interaction&#58; A review</a> <br/>
    - <a href="https://arxiv.org/abs/2302.12247">Quantifying & Modeling Multimodal Interactions&#58; An Information Decomposition Framework</a> <br/>
    - <a href="https://aclanthology.org/2020.emnlp-main.62/">Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!</a> <br/>

- date: 3/13
  title: >
    Week 6 <strong>Discussion 3: Multimodal alignment</strong>
  slides:
  topics:
  readings:
    - <a href="https://arxiv.org/pdf/2405.07987">The Platonic Representation Hypothesis</a> <br/>
    - <a href="https://arxiv.org/abs/2005.10243">What Makes for Good Views for Contrastive Learning?</a> <br/>
    - <a href="https://arxiv.org/pdf/2502.16282"> Understanding the Emergence of Multimodal Representation Alignment</a> <br/>
    - <a href="https://arxiv.org/abs/2410.23179"> Does equivariance matter at scale?</a> <br/>
    - <a href="https://arxiv.org/pdf/2103.00020"> Learning Transferable Visual Models From Natural Language Supervision?</a> <br/>
    - <a href="https://arxiv.org/pdf/2104.14294"> Emerging Properties in Self-Supervised Vision Transformers</a> <br/>
    - <a href="https://arxiv.org/abs/2209.03430"> Foundations & trends in multimodal machine learning - Principles, challenges, and open questions </a> <br/>

- date: 3/18
  title: >
    Week 7 <strong>Multimodal 3: Cross-modal transfer</strong> <a href="lec6 - crossmodal.pdf">[slides]</a>
  slides:
  topics:
    - Cross-modal learning via fusion <br/>
    - Cross-modal learning via alignment <br/>
    - Cross-modal learning via translation
  readings:
    - <a href="https://arxiv.org/abs/2303.00915v3">LLaVA-Med&#58; Training a Large Language-and-Vision Assistant for Biomedicine in One Day</a> <br/>
    - <a href="https://arxiv.org/abs/2309.11499">DreamLLM&#58; Synergistic Multimodal Comprehension and Creation</a> <br/>
    - <a href="https://arxiv.org/abs/2303.03378">PaLM-E&#58; An Embodied Multimodal Language Model</a> <br/>

- date: 3/20
  title: >
    Week 7 <strong>Discussion 4: Multimodal interactions</strong>
  slides:
  topics:
  readings:
    - <a href="https://dl.acm.org/doi/pdf/10.1145/319382.319398">Ten Myths of Multimodal Interaction</a> <br/>
    - <a href="https://www.sciencedirect.com/science/article/pii/S0167865513002584">Multimodal interaction&#58; A review</a> <br/>
    - <a href="https://arxiv.org/abs/2302.12247">Quantifying & Modeling Multimodal Interactions&#58; An Information Decomposition Framework</a> <br/>
    - <a href="https://aclanthology.org/2020.emnlp-main.62/">Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!</a> <br/>
    - <a href="https://arxiv.org/abs/2306.14824">Kosmos-2&#58; Grounding Multimodal Large Language Models to the World</a> <br/>
    - <a href="https://arxiv.org/abs/2405.09818">Chameleon&#58; Mixed-modal early-fusion foundation models</a> <br/>
    - <a href="https://link.springer.com/chapter/10.1007/978-3-031-73397-0_18">MM1&#58; Methods, Analysis and Insights from Multimodal LLM Pre-training</a><br/>
    - <a href="https://arxiv.org/pdf/2407.21770">MoMa&#58; Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts</a> <br/>

- date: 3/25
  title: >
    Week 8 <strong>No class, spring break</strong>
  slides:
  topics:
  readings:

- date: 4/1
  title: >
    Week 9 <strong>Large models 1: Large Foundation Models</strong>
  slides:
  topics:
   - Pre-training data <br/>
   - Self-supervised learning <br/>
   - Fine-tuning, instructing, alignment
  readings:

- date: 4/3
  title: >
    Week 9 <strong>Project midterm presentations</strong>
  slides:
  topics:
  readings:

- date: 4/8
  title: >
    Week 10 <strong>No class, member's week</strong>
  slides:
  topics:
  readings:

- date: 4/15
  title: >
    Week 11 <strong>Large models 2: Large multimodal models</strong>
  slides:
  topics:
    - Multimodal pre-training <br/>
    - Adapting large language models to multimodal <br/>
    - Multimodal LLMs with generation
  readings:

- date: 4/17
  title: >
    Week 11 <strong>Discussion 5: Large language models</strong>
  slides:
  topics:
  readings:

- date: 4/22
  title: >
    Week 12 <strong>Large models 3: Modern generative models</strong>
  slides:
  topics:
    - Diffusion models <br/>
    - Controllable generation
  readings:

- date: 4/24
  title: >
    Week 12 <strong>Discussion 6: Large multimodal models</strong>
  slides:
  topics:
  readings:

- date: 4/29
  title: >
    Week 13 <strong>Interaction 1: Interactive agents and reasoning</strong>
  slides:
  topics:
    - WebAgent platforms <br/>
    - Multi-step reasoning
  readings:

- date: 5/1
  title: >
    Week 13 <strong>Discussion 7: Generative AI</strong>
  slides:
  topics:
  readings:

- date: 5/6
  title: >
    Week 14 <strong>Interaction 2: Embodied AI</strong>
  slides:
  topics:
    - Reinforcement learning <br/>
    - Tangible and embodied systems <br/>
    - Real-world considerations
  readings:

- date: 5/8
  title: >
    Week 14 <strong>Project final presentations</strong>
  slides:
  topics:
  readings:

- date: 5/13
  title: >
    Week 15 <strong>Interaction 3: Human AI interaction</strong>
  slides:
  topics:
    - Interaction mediums <br/>
    - Human in the loop learning <br/>
    - Safety and reliability
  readings:
